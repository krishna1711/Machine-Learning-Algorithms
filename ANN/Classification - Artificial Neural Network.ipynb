{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "* ### Regression\n",
    "* ### Classification\n",
    "\n",
    "###### Source : https://www.youtube.com/watch?v=OTTOglLJxLU&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi&index=18 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main focus is to get started with Keras\n",
    "> #### Classification problem : \n",
    "This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. \n",
    "Many who yet to start with Deep learning and thinking how and when to start, this could a starter for you. \n",
    "<h3 style='color:#1d057d'>Just Start it guyz.. belive me its not tough</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## last feature is Exited that goes into y and first two columns are not required\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "X = data.iloc[:,3:13]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category features need to be onehot encoding so we use get dummies\n",
    "* *Note*: drop_first=True -> keeps k-1 categories instead of k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(X[['Geography', 'Gender']],drop_first=True)\n",
    "X = pd.concat([X,dummies],axis=1)\n",
    "X.drop(['Geography', 'Gender'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "> ##### Reasons to do feature scaling \n",
    ">* As we have to train data and neuron output is = W*X + B \n",
    ">* W -> Weight, X-> x data , B-> Bias\n",
    ">* As X val is higher the computation time is high and leading to lots of time in converging to global minima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU,ReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annClassifier(no_hidden_layers,neuron_list,input_dim,activation='relu',kernel_initializer='he_normal',optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']):\n",
    "    if no_hidden_layers != len(neuron_list):\n",
    "        raise ValueError('[no_hidden_layers] and length of [neuron_list] do not match.')\n",
    "    classifier = Sequential()\n",
    "    for layers_indx in range(no_hidden_layers):\n",
    "        if layers_indx ==0:\n",
    "            classifier.add(Dense(units=neuron_list[layers_indx],kernel_initializer=kernel_initializer,activation=activation,input_dim=input_dim))\n",
    "        else:\n",
    "            classifier.add(Dense(units=neuron_list[layers_indx],kernel_initializer=kernel_initializer,activation=activation))\n",
    "    classifier.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont panic the above is simple function, with respect to binary classification problem.\n",
    "##### Lemme break down the above code\n",
    "1. Parameters \n",
    "> * *no_hidden_layers*: You can specify the number of hidden layers you want. Integer value, example:  4  \n",
    "> * neuron_list: Number of neurons you want in each layer in sequence. eaxmple: [4,5,10] - Means 4,5,10 neurons in Hidden layer 1,2,3\n",
    "> * input_dim: Number of features in you X(independent features), example : in our data its 11\n",
    "> * activation='relu': Activation function\n",
    "> * kernel_initializer='he_normal'\n",
    "> * optimizer='adam'\n",
    "> * loss='binary_crossentropy'\n",
    "> * metrics=['accuracy'])\n",
    "\n",
    "2. Sequential() - For most of the models you require Sequential on which you further add your layers\n",
    "3. Dense()\n",
    "> ##### Notes:   \n",
    "> * At this point or any point in time to know the summary of the classifier/optimizer\n",
    "> * classifier.summary()\n",
    "> * optimizer now adam is best\n",
    "> * if output is binary loss -> binary_crossentropy, multiple class, loss -> categoricalcrossentropy \n",
    "\n",
    "\n",
    "###### Note: Everything is put into a python function else we can also define custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below fucntion is also as same the above function except for Dropouts, we have added Dropout for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annClassifier(no_hidden_layers,neuron_list,input_dim,activation='relu',kernel_initializer='he_normal',optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']):\n",
    "    dropOuts = [0.2,0.4,0.3,0.6]   \n",
    "    if no_hidden_layers != len(neuron_list):\n",
    "        raise ValueError('[no_hidden_layers] and length of [neuron_list] do not match.')\n",
    "    classifier = Sequential()\n",
    "    for layers_indx in range(no_hidden_layers):\n",
    "        if layers_indx ==0:\n",
    "            classifier.add(Dense(units=neuron_list[layers_indx],kernel_initializer=kernel_initializer,activation=activation,input_dim=input_dim))\n",
    "            classifier.add(Dropout(random.choice(dropOuts)))\n",
    "        else:\n",
    "            classifier.add(Dense(units=neuron_list[layers_indx],kernel_initializer=kernel_initializer,activation=activation))\n",
    "            classifier.add(Dropout(random.choice(dropOuts)))\n",
    "    classifier.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we go... Try it out with different combination of neurons and hidden layers. Soon we will look into hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5359 samples, validate on 2641 samples\n",
      "Epoch 1/100\n",
      "5359/5359 [==============================] - 1s 150us/step - loss: 2.8075 - accuracy: 0.7731 - val_loss: 2.4547 - val_accuracy: 0.7758\n",
      "Epoch 2/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 2.3620 - accuracy: 0.7791 - val_loss: 1.9318 - val_accuracy: 0.7846\n",
      "Epoch 3/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 1.3773 - accuracy: 0.7589 - val_loss: 1.1171 - val_accuracy: 0.7724\n",
      "Epoch 4/100\n",
      "5359/5359 [==============================] - 1s 116us/step - loss: 0.9703 - accuracy: 0.7673 - val_loss: 0.8749 - val_accuracy: 0.7736\n",
      "Epoch 5/100\n",
      "5359/5359 [==============================] - 1s 115us/step - loss: 0.8437 - accuracy: 0.7714 - val_loss: 0.7809 - val_accuracy: 0.7849\n",
      "Epoch 6/100\n",
      "5359/5359 [==============================] - 1s 118us/step - loss: 0.7722 - accuracy: 0.7494 - val_loss: 0.7084 - val_accuracy: 0.7357\n",
      "Epoch 7/100\n",
      "5359/5359 [==============================] - 1s 116us/step - loss: 0.6785 - accuracy: 0.7729 - val_loss: 0.6870 - val_accuracy: 0.7774\n",
      "Epoch 8/100\n",
      "5359/5359 [==============================] - 1s 116us/step - loss: 0.7254 - accuracy: 0.7363 - val_loss: 0.7026 - val_accuracy: 0.7474\n",
      "Epoch 9/100\n",
      "5359/5359 [==============================] - 1s 108us/step - loss: 0.5923 - accuracy: 0.7541 - val_loss: 0.6035 - val_accuracy: 0.7815\n",
      "Epoch 10/100\n",
      "5359/5359 [==============================] - 1s 113us/step - loss: 0.5810 - accuracy: 0.7815 - val_loss: 0.6338 - val_accuracy: 0.7929\n",
      "Epoch 11/100\n",
      "5359/5359 [==============================] - 1s 115us/step - loss: 0.5643 - accuracy: 0.7962 - val_loss: 0.6249 - val_accuracy: 0.8012\n",
      "Epoch 12/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.5361 - accuracy: 0.8005 - val_loss: 0.5878 - val_accuracy: 0.7989\n",
      "Epoch 13/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.5102 - accuracy: 0.8013 - val_loss: 0.5666 - val_accuracy: 0.8001\n",
      "Epoch 14/100\n",
      "5359/5359 [==============================] - 1s 113us/step - loss: 0.4970 - accuracy: 0.7975 - val_loss: 0.5595 - val_accuracy: 0.8027\n",
      "Epoch 15/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.4932 - accuracy: 0.8052 - val_loss: 0.5518 - val_accuracy: 0.8050\n",
      "Epoch 16/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.5063 - accuracy: 0.8061 - val_loss: 0.5739 - val_accuracy: 0.8080\n",
      "Epoch 17/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.4779 - accuracy: 0.8149 - val_loss: 0.5241 - val_accuracy: 0.8118\n",
      "Epoch 18/100\n",
      "5359/5359 [==============================] - 1s 111us/step - loss: 0.4663 - accuracy: 0.8175 - val_loss: 0.5456 - val_accuracy: 0.8126\n",
      "Epoch 19/100\n",
      "5359/5359 [==============================] - 1s 120us/step - loss: 0.4699 - accuracy: 0.8136 - val_loss: 0.5072 - val_accuracy: 0.8148\n",
      "Epoch 20/100\n",
      "5359/5359 [==============================] - 1s 117us/step - loss: 0.4559 - accuracy: 0.8192 - val_loss: 0.5025 - val_accuracy: 0.8111\n",
      "Epoch 21/100\n",
      "5359/5359 [==============================] - 1s 115us/step - loss: 0.4499 - accuracy: 0.8242 - val_loss: 0.4649 - val_accuracy: 0.8133\n",
      "Epoch 22/100\n",
      "5359/5359 [==============================] - 1s 118us/step - loss: 0.4754 - accuracy: 0.8119 - val_loss: 0.7193 - val_accuracy: 0.8065\n",
      "Epoch 23/100\n",
      "5359/5359 [==============================] - 1s 123us/step - loss: 0.5195 - accuracy: 0.8147 - val_loss: 0.4825 - val_accuracy: 0.8111\n",
      "Epoch 24/100\n",
      "5359/5359 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.82 - 1s 123us/step - loss: 0.4549 - accuracy: 0.8240 - val_loss: 0.4747 - val_accuracy: 0.8156\n",
      "Epoch 25/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.4432 - accuracy: 0.8296 - val_loss: 0.4628 - val_accuracy: 0.8111\n",
      "Epoch 26/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.4504 - accuracy: 0.8071 - val_loss: 0.4675 - val_accuracy: 0.8152\n",
      "Epoch 27/100\n",
      "5359/5359 [==============================] - 1s 108us/step - loss: 0.4062 - accuracy: 0.8270 - val_loss: 0.5026 - val_accuracy: 0.8186\n",
      "Epoch 28/100\n",
      "5359/5359 [==============================] - 1s 103us/step - loss: 0.4222 - accuracy: 0.8328 - val_loss: 0.4652 - val_accuracy: 0.8239\n",
      "Epoch 29/100\n",
      "5359/5359 [==============================] - 1s 104us/step - loss: 0.3988 - accuracy: 0.8339 - val_loss: 0.4413 - val_accuracy: 0.8217\n",
      "Epoch 30/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.4014 - accuracy: 0.8356 - val_loss: 0.4636 - val_accuracy: 0.8311\n",
      "Epoch 31/100\n",
      "5359/5359 [==============================] - 1s 123us/step - loss: 0.4029 - accuracy: 0.8354 - val_loss: 0.4954 - val_accuracy: 0.8304\n",
      "Epoch 32/100\n",
      "5359/5359 [==============================] - 1s 113us/step - loss: 0.4380 - accuracy: 0.8270 - val_loss: 0.5354 - val_accuracy: 0.8236\n",
      "Epoch 33/100\n",
      "5359/5359 [==============================] - 1s 122us/step - loss: 0.5031 - accuracy: 0.8104 - val_loss: 0.4498 - val_accuracy: 0.8258\n",
      "Epoch 34/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.4666 - accuracy: 0.8149 - val_loss: 0.4752 - val_accuracy: 0.8281\n",
      "Epoch 35/100\n",
      "5359/5359 [==============================] - 1s 138us/step - loss: 0.4534 - accuracy: 0.8263 - val_loss: 0.4522 - val_accuracy: 0.8129\n",
      "Epoch 36/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.4275 - accuracy: 0.8364 - val_loss: 0.4620 - val_accuracy: 0.8281\n",
      "Epoch 37/100\n",
      "5359/5359 [==============================] - 1s 114us/step - loss: 0.4106 - accuracy: 0.8408 - val_loss: 0.4865 - val_accuracy: 0.8289\n",
      "Epoch 38/100\n",
      "5359/5359 [==============================] - 1s 121us/step - loss: 0.4050 - accuracy: 0.8433 - val_loss: 0.4487 - val_accuracy: 0.8296\n",
      "Epoch 39/100\n",
      "5359/5359 [==============================] - 1s 125us/step - loss: 0.3921 - accuracy: 0.8403 - val_loss: 0.4635 - val_accuracy: 0.8353\n",
      "Epoch 40/100\n",
      "5359/5359 [==============================] - 1s 129us/step - loss: 0.3923 - accuracy: 0.8416 - val_loss: 0.4559 - val_accuracy: 0.8353\n",
      "Epoch 41/100\n",
      "5359/5359 [==============================] - 1s 137us/step - loss: 0.3938 - accuracy: 0.8410 - val_loss: 0.5911 - val_accuracy: 0.8243\n",
      "Epoch 42/100\n",
      "5359/5359 [==============================] - 1s 145us/step - loss: 0.4043 - accuracy: 0.8425 - val_loss: 0.4684 - val_accuracy: 0.8349\n",
      "Epoch 43/100\n",
      "5359/5359 [==============================] - 1s 153us/step - loss: 0.3893 - accuracy: 0.8419 - val_loss: 0.4417 - val_accuracy: 0.8383\n",
      "Epoch 44/100\n",
      "5359/5359 [==============================] - 1s 122us/step - loss: 0.3730 - accuracy: 0.8474 - val_loss: 0.4601 - val_accuracy: 0.8357\n",
      "Epoch 45/100\n",
      "5359/5359 [==============================] - 1s 125us/step - loss: 0.3761 - accuracy: 0.8461 - val_loss: 0.4533 - val_accuracy: 0.8345\n",
      "Epoch 46/100\n",
      "5359/5359 [==============================] - 1s 127us/step - loss: 0.4042 - accuracy: 0.8334 - val_loss: 0.4549 - val_accuracy: 0.8376\n",
      "Epoch 47/100\n",
      "5359/5359 [==============================] - 1s 121us/step - loss: 0.3833 - accuracy: 0.8468 - val_loss: 0.4688 - val_accuracy: 0.8387\n",
      "Epoch 48/100\n",
      "5359/5359 [==============================] - 1s 131us/step - loss: 0.3700 - accuracy: 0.8464 - val_loss: 0.4373 - val_accuracy: 0.8326\n",
      "Epoch 49/100\n",
      "5359/5359 [==============================] - 1s 126us/step - loss: 0.3688 - accuracy: 0.8447 - val_loss: 0.4380 - val_accuracy: 0.8383\n",
      "Epoch 50/100\n",
      "5359/5359 [==============================] - 1s 136us/step - loss: 0.3690 - accuracy: 0.8431 - val_loss: 0.4701 - val_accuracy: 0.8406\n",
      "Epoch 51/100\n",
      "5359/5359 [==============================] - 1s 126us/step - loss: 0.3680 - accuracy: 0.8390 - val_loss: 0.4592 - val_accuracy: 0.8357\n",
      "Epoch 52/100\n",
      "5359/5359 [==============================] - 1s 126us/step - loss: 0.3685 - accuracy: 0.8436 - val_loss: 0.4547 - val_accuracy: 0.8368\n",
      "Epoch 53/100\n",
      "5359/5359 [==============================] - 1s 120us/step - loss: 0.3664 - accuracy: 0.8447 - val_loss: 0.4707 - val_accuracy: 0.8391\n",
      "Epoch 54/100\n",
      "5359/5359 [==============================] - 1s 116us/step - loss: 0.3853 - accuracy: 0.8362 - val_loss: 0.4590 - val_accuracy: 0.8266\n",
      "Epoch 55/100\n",
      "5359/5359 [==============================] - 1s 121us/step - loss: 0.3716 - accuracy: 0.8414 - val_loss: 0.4906 - val_accuracy: 0.8319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "5359/5359 [==============================] - 1s 127us/step - loss: 0.3756 - accuracy: 0.8399 - val_loss: 0.4479 - val_accuracy: 0.8281\n",
      "Epoch 57/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.4019 - accuracy: 0.8371 - val_loss: 0.4585 - val_accuracy: 0.8296\n",
      "Epoch 58/100\n",
      "5359/5359 [==============================] - 1s 117us/step - loss: 0.3711 - accuracy: 0.8425 - val_loss: 0.4496 - val_accuracy: 0.8379\n",
      "Epoch 59/100\n",
      "5359/5359 [==============================] - 1s 117us/step - loss: 0.3548 - accuracy: 0.8436 - val_loss: 0.4491 - val_accuracy: 0.8391\n",
      "Epoch 60/100\n",
      "5359/5359 [==============================] - 1s 111us/step - loss: 0.3644 - accuracy: 0.8444 - val_loss: 0.4615 - val_accuracy: 0.8334\n",
      "Epoch 61/100\n",
      "5359/5359 [==============================] - 1s 112us/step - loss: 0.3525 - accuracy: 0.8446 - val_loss: 0.4542 - val_accuracy: 0.8368\n",
      "Epoch 62/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.3461 - accuracy: 0.8459 - val_loss: 0.4614 - val_accuracy: 0.8353\n",
      "Epoch 63/100\n",
      "5359/5359 [==============================] - 1s 120us/step - loss: 0.3489 - accuracy: 0.8427 - val_loss: 0.4865 - val_accuracy: 0.8319\n",
      "Epoch 64/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.3494 - accuracy: 0.8427 - val_loss: 0.4750 - val_accuracy: 0.8387\n",
      "Epoch 65/100\n",
      "5359/5359 [==============================] - 1s 110us/step - loss: 0.3458 - accuracy: 0.8442 - val_loss: 0.4737 - val_accuracy: 0.8353\n",
      "Epoch 66/100\n",
      "5359/5359 [==============================] - 1s 111us/step - loss: 0.3412 - accuracy: 0.8440 - val_loss: 0.4718 - val_accuracy: 0.8368\n",
      "Epoch 67/100\n",
      "5359/5359 [==============================] - 1s 114us/step - loss: 0.3466 - accuracy: 0.8455 - val_loss: 0.4669 - val_accuracy: 0.8406\n",
      "Epoch 68/100\n",
      "5359/5359 [==============================] - 1s 144us/step - loss: 0.3550 - accuracy: 0.8434 - val_loss: 0.4640 - val_accuracy: 0.8391\n",
      "Epoch 69/100\n",
      "5359/5359 [==============================] - 1s 124us/step - loss: 0.3420 - accuracy: 0.8472 - val_loss: 0.4686 - val_accuracy: 0.8402\n",
      "Epoch 70/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.3416 - accuracy: 0.8487 - val_loss: 0.4717 - val_accuracy: 0.8353\n",
      "Epoch 71/100\n",
      "5359/5359 [==============================] - 1s 108us/step - loss: 0.3692 - accuracy: 0.8406 - val_loss: 0.4694 - val_accuracy: 0.8319\n",
      "Epoch 72/100\n",
      "5359/5359 [==============================] - 1s 111us/step - loss: 0.3516 - accuracy: 0.8477 - val_loss: 0.4677 - val_accuracy: 0.8451\n",
      "Epoch 73/100\n",
      "5359/5359 [==============================] - 1s 112us/step - loss: 0.3465 - accuracy: 0.8483 - val_loss: 0.4709 - val_accuracy: 0.8406\n",
      "Epoch 74/100\n",
      "5359/5359 [==============================] - 1s 111us/step - loss: 0.3421 - accuracy: 0.8513 - val_loss: 0.4576 - val_accuracy: 0.8387\n",
      "Epoch 75/100\n",
      "5359/5359 [==============================] - 1s 118us/step - loss: 0.3373 - accuracy: 0.8531 - val_loss: 0.4583 - val_accuracy: 0.8429\n",
      "Epoch 76/100\n",
      "5359/5359 [==============================] - 1s 123us/step - loss: 0.3302 - accuracy: 0.8559 - val_loss: 0.4775 - val_accuracy: 0.8459\n",
      "Epoch 77/100\n",
      "5359/5359 [==============================] - 1s 140us/step - loss: 0.3459 - accuracy: 0.8528 - val_loss: 0.4451 - val_accuracy: 0.8410\n",
      "Epoch 78/100\n",
      "5359/5359 [==============================] - 1s 133us/step - loss: 0.3370 - accuracy: 0.8526 - val_loss: 0.4697 - val_accuracy: 0.8432\n",
      "Epoch 79/100\n",
      "5359/5359 [==============================] - 1s 120us/step - loss: 0.3232 - accuracy: 0.8537 - val_loss: 0.4538 - val_accuracy: 0.8391\n",
      "Epoch 80/100\n",
      "5359/5359 [==============================] - 1s 124us/step - loss: 0.3250 - accuracy: 0.8543 - val_loss: 0.4697 - val_accuracy: 0.8391\n",
      "Epoch 81/100\n",
      "5359/5359 [==============================] - 1s 132us/step - loss: 0.3386 - accuracy: 0.8494 - val_loss: 0.4684 - val_accuracy: 0.8402\n",
      "Epoch 82/100\n",
      "5359/5359 [==============================] - 1s 115us/step - loss: 0.3265 - accuracy: 0.8530 - val_loss: 0.4794 - val_accuracy: 0.8429\n",
      "Epoch 83/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.3259 - accuracy: 0.8530 - val_loss: 0.4832 - val_accuracy: 0.8455\n",
      "Epoch 84/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.3664 - accuracy: 0.8548 - val_loss: 0.4933 - val_accuracy: 0.8459\n",
      "Epoch 85/100\n",
      "5359/5359 [==============================] - 1s 121us/step - loss: 0.3303 - accuracy: 0.8552 - val_loss: 0.4494 - val_accuracy: 0.8432\n",
      "Epoch 86/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.3267 - accuracy: 0.8572 - val_loss: 0.4611 - val_accuracy: 0.8436\n",
      "Epoch 87/100\n",
      "5359/5359 [==============================] - 1s 119us/step - loss: 0.3230 - accuracy: 0.8524 - val_loss: 0.4646 - val_accuracy: 0.8432\n",
      "Epoch 88/100\n",
      "5359/5359 [==============================] - 1s 114us/step - loss: 0.3340 - accuracy: 0.8543 - val_loss: 0.4622 - val_accuracy: 0.8448\n",
      "Epoch 89/100\n",
      "5359/5359 [==============================] - 1s 109us/step - loss: 0.3264 - accuracy: 0.8561 - val_loss: 0.4614 - val_accuracy: 0.8478\n",
      "Epoch 90/100\n",
      "5359/5359 [==============================] - 1s 118us/step - loss: 0.3159 - accuracy: 0.8563 - val_loss: 0.4781 - val_accuracy: 0.8440\n",
      "Epoch 91/100\n",
      "5359/5359 [==============================] - 1s 120us/step - loss: 0.3209 - accuracy: 0.8531 - val_loss: 0.4757 - val_accuracy: 0.8410\n",
      "Epoch 92/100\n",
      "5359/5359 [==============================] - 1s 122us/step - loss: 0.3402 - accuracy: 0.8537 - val_loss: 0.4566 - val_accuracy: 0.8440\n",
      "Epoch 93/100\n",
      "5359/5359 [==============================] - 1s 130us/step - loss: 0.3319 - accuracy: 0.8546 - val_loss: 0.4325 - val_accuracy: 0.8429\n",
      "Epoch 94/100\n",
      "5359/5359 [==============================] - 1s 141us/step - loss: 0.3320 - accuracy: 0.8546 - val_loss: 0.4271 - val_accuracy: 0.8406\n",
      "Epoch 95/100\n",
      "5359/5359 [==============================] - 1s 144us/step - loss: 0.3344 - accuracy: 0.8524 - val_loss: 0.4363 - val_accuracy: 0.8432\n",
      "Epoch 96/100\n",
      "5359/5359 [==============================] - 1s 143us/step - loss: 0.3271 - accuracy: 0.8558 - val_loss: 0.4479 - val_accuracy: 0.8440\n",
      "Epoch 97/100\n",
      "5359/5359 [==============================] - 1s 160us/step - loss: 0.3218 - accuracy: 0.8571 - val_loss: 0.4545 - val_accuracy: 0.8451\n",
      "Epoch 98/100\n",
      "5359/5359 [==============================] - 1s 160us/step - loss: 0.3183 - accuracy: 0.8567 - val_loss: 0.4505 - val_accuracy: 0.8455\n",
      "Epoch 99/100\n",
      "5359/5359 [==============================] - 1s 126us/step - loss: 0.3257 - accuracy: 0.8569 - val_loss: 0.4812 - val_accuracy: 0.8459\n",
      "Epoch 100/100\n",
      "5359/5359 [==============================] - 1s 123us/step - loss: 0.3253 - accuracy: 0.8543 - val_loss: 0.4591 - val_accuracy: 0.8413\n"
     ]
    }
   ],
   "source": [
    "annoptimizer = annClassifier(no_hidden_layers=4,neuron_list=[10,20,15,1],input_dim=X.shape[-1])\n",
    "annclassifier_history = annoptimizer.fit(X_train,y_train,validation_split=0.33, batch_size=10,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1537   58]\n",
      " [ 236  169]]\n",
      "\n",
      "Accurcay: 0.853\n"
     ]
    }
   ],
   "source": [
    "## prediction\n",
    "y_pred = annoptimizer.predict(X_test)\n",
    "y_pred = (y_pred>0.5)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "## confusion matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "## accuracy score\n",
    "acc_score = accuracy_score(y_test,y_pred) ## 0.8475\n",
    "print(f\"Confusion matrix:\\n{cm}\\n\\nAccurcay: {acc_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
